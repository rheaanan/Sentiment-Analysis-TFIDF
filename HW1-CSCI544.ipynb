{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/rheaanand/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rheaanand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk    # stemming, lemmatization etc\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "import re      # for removing urls etc\n",
    "import urllib\n",
    "import contractions # won't to will not, don't to do not \n",
    "from bs4 import BeautifulSoup # remove html content \n",
    "import sklearn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install bs4 # in case you don't have it installed\n",
    "\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Kitchen_v1_00.tsv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv('data.tsv', error_bad_lines = False, sep = '\\t', warn_bad_lines=False)\n",
    "# \\t because its tsv file \n",
    "# it will throw an error if number of rows are not in correct allignment \n",
    "# error_bad_lines = False will ignore such lines, when True (default)it will throw an error\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ************ 3 Sample Reviews ***********\n",
      "\n",
      "                                               review_body  star_rating\n",
      "2250839  This was outstanding. Looked so great on my ta...          5.0\n",
      "1383289  It's a timer. It counts down time. I also use ...          4.0\n",
      "3685721  I was very satisfied the product and the speed...          5.0\n",
      "\n",
      " ************ Frequency of each rating  ************\n",
      "\n",
      "5.0    3124595\n",
      "4.0     731701\n",
      "1.0     426870\n",
      "3.0     349539\n",
      "2.0     241939\n",
      "Name: star_rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Keep rating and reviews only\n",
    "text_df = text_df[['review_body','star_rating']] \n",
    "\n",
    "# drop na rows from ratings and reviews\n",
    "text_df.dropna(subset = [\"review_body\"],inplace=True)\n",
    "text_df.dropna(subset = [\"star_rating\"],inplace=True)\n",
    "\n",
    "# Print 3 reviews\n",
    "print(\"\\n ************ 3 Sample Reviews ***********\\n\")   \n",
    "print(text_df.sample(3))\n",
    "\n",
    "print(\"\\n ************ Frequency of each rating  ************\\n\")\n",
    "\n",
    "# Print frequency of the rating\n",
    "count = text_df['star_rating'].value_counts() # how many rows for each rating 1,2,3,4,5 \n",
    "print(count)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Reviews:\n",
    "## The reviews with rating 4,5 are labelled to be 1 and 1,2 are labelled as 0. Discard the reviews with rating 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "************ Frequency of each sentiment ************\n",
      "\n",
      "positive    3856296\n",
      "negative     668809\n",
      "neutral      349539\n",
      "Name: sentiment, dtype: int64\n",
      "\n",
      "**** Frequency of each sentiment after removing neutral reviews ****\n",
      "\n",
      "positive    3856296\n",
      "negative     668809\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1/2 rating negative sentinment \n",
    "# if 3 discard because its neutral \n",
    "# if its 4/5 positive sentiment\n",
    "\n",
    "\n",
    "\n",
    "text_df['label'] = np.where(text_df[\"star_rating\"] >= 4, 1, 0)     # create positive and negative sentiment label\n",
    "text_df['sentiment'] = np.where(text_df[\"star_rating\"] >= 4, \"positive\", \"negative\")     # create positive and negative sentiment label\n",
    "text_df['label'] = np.where(text_df[\"star_rating\"] == 3, -1,text_df['label'])\n",
    "text_df['sentiment'] = np.where(text_df[\"star_rating\"] == 3, \"neutral\",text_df['sentiment'])\n",
    "text_df = text_df[['star_rating','review_body','label','sentiment']]           # copying to a new data frame \n",
    "\n",
    "\n",
    "count = text_df['sentiment'].value_counts()                         # counting frequency of each label \n",
    "print(\"\\n************ Frequency of each sentiment ************\\n\")\n",
    "print(count)\n",
    " \n",
    "text_df = text_df[text_df.star_rating != 3]                           # delete the rows with neutral rating 3.\n",
    "count = text_df['sentiment'].value_counts()                         # counting frequency of each label after neutral is dropped\n",
    "print(\"\\n**** Frequency of each sentiment after removing neutral reviews ****\\n\")\n",
    "print(count)\n",
    "text_df = text_df[['review_body','star_rating','label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We select 200000 reviews randomly with 100,000 positive and 100,000 negative reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg. character length before cleaning : 322.651215\n",
      "\n",
      " ************ 3 Sample Reviews before cleaning ***********\n",
      "\n",
      "4096630    I saw this kettle in a friend's house (she rec...\n",
      "3426962    Heavy duty and non stick. Just the right size ...\n",
      "1950150    Not what  I thought I needed but would be grea...\n",
      "Name: review_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "sm0 = text_df.label[text_df.label.eq(0)].sample(100000,random_state=80).index    #randomly select 100000 positive reviews\n",
    "sm1 = text_df.label[text_df.label.eq(1)].sample(100000, random_state=80).index    #randomly select 100000 negative reviews\n",
    "\n",
    "text_df = text_df.loc[sm0.union(sm1)]     # combine into one dataset\n",
    "\n",
    "avg_char = text_df['review_body'].apply(lambda a :len(str(a))).mean()  # finding avg no. of characters in a review\n",
    "print(\" Avg. character length before cleaning :\",avg_char)\n",
    "\n",
    "print(\"\\n ************ 3 Sample Reviews before cleaning ***********\\n\")  \n",
    "print(text_df['review_body'].sample(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "## Convert the all reviews into the lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[\"review_body\"] = text_df[\"review_body\"].str.lower()     # convert everything to lower case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the HTML and URLs from the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing html tags using beautiful soup like <br> tags\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: BeautifulSoup(str(x)).get_text()) \n",
    "\n",
    "# Removing urls from reviews\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: re.sub(r'\\s*(https?://|www\\.)+\\S+(\\s+|$)', \" \", str(x), flags=re.UNICODE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Removing Digits from the review_body \n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: re.sub(r\"[^\\D']+\", \" \", str(x), flags=re.UNICODE)) # remove all numbers\n",
    "\n",
    "# Removing Special Characters\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: re.sub(r\"[^\\w']+\", \" \", str(x), flags=re.UNICODE)) # remove all special characters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the extra spaces between the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove more than one spaces\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: re.sub(r'\\s+',' ', str(x), flags = re.UNICODE))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform contractions on the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractionfunction(s):                      \n",
    "    s = s.apply(lambda x: contractions.fix(x))\n",
    "    return s\n",
    "\n",
    "text_df_onecol = contractionfunction(text_df[\"review_body\"])\n",
    "text_df[\"review_body\"] = text_df_onecol\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].str.lower()     # convert everything to lower case again because contractions adds I \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. character length before preprocessing 309.412685\n"
     ]
    }
   ],
   "source": [
    "avg_char = text_df['review_body'].apply(lambda a :len(str(a))).mean()  # finding avg no. of characters in a review\n",
    "print(\"Avg. character length before preprocessing\", avg_char)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# storing all the stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# remove stop words from each review  \n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: \" \".join([item  for item in str(x).split() if item not in stop_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perform lemmatization  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. character length after data cleaning + preprocessing : 189.412555\n",
      "\n",
      " ************ 3 Sample Reviews after data cleaning + preprocessing ***********\n",
      "\n",
      "                                               review_body  star_rating  label\n",
      "3252743  happy drawer insert shave sand paper bit fit d...          5.0      1\n",
      "1971228  produce ton fine grind setting consistency get...          2.0      0\n",
      "4587690  bought plate also black small salad dessert pl...          1.0      0\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text_df[\"review_body\"] = text_df[\"review_body\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(item)  for item in str(x).split()]))\n",
    "\n",
    "avg_char_after = text_df['review_body'].apply(lambda x : (len(str(x)))).mean()\n",
    "print(\"Avg. character length after data cleaning + preprocessing :\", avg_char_after)\n",
    "\n",
    "print(\"\\n ************ 3 Sample Reviews after data cleaning + preprocessing ***********\\n\")  \n",
    "print(text_df.sample(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# fit and transform on train data and only transform test data respectively.\n",
    "# converting each review to a vector of max 2000 words \n",
    "# min_df specifies min frequency of a word selected as a feature i.e the word has to occur atleast once \n",
    "# max_df ensures that a word used in more than 70% of the reviews is not considered as a feature\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "#Split train and test into 80 20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_df[\"review_body\"],text_df[\"label\"], test_size=0.2, random_state=90)\n",
    "\n",
    "tfidfconverter = TfidfVectorizer(max_features=2000, min_df=1, max_df=0.7)\n",
    "\n",
    "# fit decides the features based on the train dataset whose retrictions were described in the above TfidfVectorizer function \n",
    "X_train = tfidfconverter.fit_transform(X_train)\n",
    "X_test = tfidfconverter.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ************ Evaluation metrics on training data ***********\n",
      "\n",
      "Training Accuracy: 0.84600\n",
      "Training F1 Score: 0.84675\n",
      "Training Precision Score: 0.84122\n",
      "Training Recall Score: 0.85235\n",
      "\n",
      " ************ Evaluation metrics on test data ***********\n",
      "\n",
      "Testing Accuracy: 0.84325\n",
      "Testing F1 Score: 0.84506\n",
      "Testing Precision Score: 0.84107\n",
      "Testing Recall Score: 0.84909\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "\n",
    "# standard scalera is a function used to normalize the review vectors \n",
    "sc = StandardScaler(with_mean=False)\n",
    "\n",
    "# using the normalizing function to create a normalized training dataset\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "\n",
    "# normalize the test data using the same scaler\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# Create a perceptron object with the parameters: 40 iterations (epochs) over the data, and a learning rate of 0.1\n",
    "ppn = Perceptron(max_iter=100, eta0=0.1, random_state=0)\n",
    "\n",
    "# Train the perceptron\n",
    "ppn.fit(X_train_std, y_train)\n",
    "\n",
    "\n",
    "print(\"\\n ************ Evaluation metrics on training data ***********\\n\") \n",
    "\n",
    "y_pred_train = ppn.predict(X_train_std)\n",
    "\n",
    "print('Training Accuracy: %.5f' % accuracy_score(y_train, y_pred_train))\n",
    "print('Training F1 Score: %.5f' % f1_score(y_train, y_pred_train))\n",
    "print('Training Precision Score: %.5f' % precision_score(y_train, y_pred_train))\n",
    "print('Training Recall Score: %.5f' % recall_score(y_train, y_pred_train))\n",
    "\n",
    "y_pred_test = ppn.predict(X_test_std)\n",
    "\n",
    "print(\"\\n ************ Evaluation metrics on test data ***********\\n\") \n",
    "\n",
    "print('Testing Accuracy: %.5f' % accuracy_score(y_test, y_pred_test))\n",
    "print('Testing F1 Score: %.5f' % f1_score(y_test, y_pred_test))\n",
    "print('Testing Precision Score: %.5f' % precision_score(y_test, y_pred_test))\n",
    "print('Testing Recall Score: %.5f' % recall_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ************ Evaluation metrics on training data ***********\n",
      "\n",
      "Training Accuracy: 0.89763\n",
      "Training F1 Score: 0.89708\n",
      "Training Precision Score: 0.90037\n",
      "Training Recall Score: 0.89382\n",
      "\n",
      " ************ Evaluation metrics on test data ***********\n",
      "\n",
      "Testing Accuracy: 0.89470\n",
      "Testing F1 Score: 0.89523\n",
      "Testing Precision Score: 0.89684\n",
      "Testing Recall Score: 0.89363\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import svm\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "svm_clf = svm.LinearSVC() # Linear Kernel\n",
    "\n",
    "#Train the model using the training sets\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n ************ Evaluation metrics on training data ***********\\n\") \n",
    "\n",
    "y_pred_train = svm_clf.predict(X_train)\n",
    "\n",
    "print('Training Accuracy: %.5f' % accuracy_score(y_train, y_pred_train))\n",
    "print('Training F1 Score: %.5f' % f1_score(y_train, y_pred_train))\n",
    "print('Training Precision Score: %.5f' % precision_score(y_train, y_pred_train))\n",
    "print('Training Recall Score: %.5f' % recall_score(y_train, y_pred_train))\n",
    "\n",
    "y_pred_test = svm_clf.predict(X_test)\n",
    "\n",
    "print(\"\\n ************ Evaluation metrics on test data ***********\\n\") \n",
    "\n",
    "print('Testing Accuracy: %.5f' % accuracy_score(y_test, y_pred_test))\n",
    "print('Testing F1 Score: %.5f' % f1_score(y_test, y_pred_test))\n",
    "print('Testing Precision Score: %.5f' % precision_score(y_test, y_pred_test))\n",
    "print('Testing Recall Score: %.5f' % recall_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ************ Evaluation metrics on training data ***********\n",
      "\n",
      "Training Accuracy: 0.89706\n",
      "Training F1 Score: 0.89635\n",
      "Training Precision Score: 0.90100\n",
      "Training Recall Score: 0.89174\n",
      "\n",
      " ************ Evaluation metrics on test data ***********\n",
      "\n",
      "Testing Accuracy: 0.89570\n",
      "Testing F1 Score: 0.89606\n",
      "Testing Precision Score: 0.89911\n",
      "Testing Recall Score: 0.89304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(\"\\n ************ Evaluation metrics on training data ***********\\n\") \n",
    "\n",
    "y_pred_train = logreg.predict(X_train)\n",
    "\n",
    "print('Training Accuracy: %.5f' % accuracy_score(y_train, y_pred_train))\n",
    "print('Training F1 Score: %.5f' % f1_score(y_train, y_pred_train))\n",
    "print('Training Precision Score: %.5f' % precision_score(y_train, y_pred_train))\n",
    "print('Training Recall Score: %.5f' % recall_score(y_train, y_pred_train))\n",
    "\n",
    "y_pred_test = logreg.predict(X_test)\n",
    "\n",
    "print(\"\\n ************ Evaluation metrics on test data ***********\\n\") \n",
    "\n",
    "print('Testing Accuracy: %.5f' % accuracy_score(y_test, y_pred_test))\n",
    "print('Testing F1 Score: %.5f' % f1_score(y_test, y_pred_test))\n",
    "print('Testing Precision Score: %.5f' % precision_score(y_test, y_pred_test))\n",
    "print('Testing Recall Score: %.5f' % recall_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ************ Evaluation metrics on training data ***********\n",
      "\n",
      "Training Accuracy: 0.86606\n",
      "Training F1 Score: 0.86594\n",
      "Training Precision Score: 0.86526\n",
      "Training Recall Score: 0.86661\n",
      "\n",
      " ************ Evaluation metrics on test data ***********\n",
      "\n",
      "Testing Accuracy: 0.86460\n",
      "Testing F1 Score: 0.86583\n",
      "Testing Precision Score: 0.86387\n",
      "Testing Recall Score: 0.86781\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Create a Multinomial Naive Bayes model with default parameters \n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the model using the training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Predict Output\n",
    "y_pred = model.predict(X_test) # 0:Overcast, 2:Mild\n",
    "\n",
    "\n",
    "print(\"\\n ************ Evaluation metrics on training data ***********\\n\") \n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "print('Training Accuracy: %.5f' % accuracy_score(y_train, y_pred_train))\n",
    "print('Training F1 Score: %.5f' % f1_score(y_train, y_pred_train))\n",
    "print('Training Precision Score: %.5f' % precision_score(y_train, y_pred_train))\n",
    "print('Training Recall Score: %.5f' % recall_score(y_train, y_pred_train))\n",
    "\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"\\n ************ Evaluation metrics on test data ***********\\n\") \n",
    "\n",
    "print('Testing Accuracy: %.5f' % accuracy_score(y_test, y_pred_test))\n",
    "print('Testing F1 Score: %.5f' % f1_score(y_test, y_pred_test))\n",
    "print('Testing Precision Score: %.5f' % precision_score(y_test, y_pred_test))\n",
    "print('Testing Recall Score: %.5f' % recall_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
